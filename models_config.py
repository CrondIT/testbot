"""Configuration for AI models used by the bot."""

import os
import io
from dotenv import load_dotenv

from google import genai
from openai import OpenAI

import base64

import token_utils

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ .env
load_dotenv()

# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
OPENAI_API_KEY_CHAT = os.getenv("OPENAI_API_KEY")
OPENAI_API_KEY_IMAGE = os.getenv("OPENAI_API_KEY_IMAGE")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ OpenAI –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
client_chat = OpenAI(api_key=OPENAI_API_KEY_CHAT)
client_image = OpenAI(api_key=OPENAI_API_KEY_IMAGE)
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Gemini
client_edit_image = genai.Client(api_key=GEMINI_API_KEY)

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
MODELS = {
    "chat": "gpt-5.2-chat-latest",
    "image": "dall-e-3",
    "edit": "gemini-2.5-flash-image",
    "ai_file": "gpt-5.2-chat-latest",
}

SYSTEM_PROMPTS = {
    "chat": (
        "You are a helpful assistant. "
        "Use web search only when your knowledge may be outdated "
        "or when the user explicitly asks for fresh data."
    ),
    "image": ("–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."),
    "edit": ("–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Gemini."),
    "ai_file": (
        "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
        "–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–∞—Å–∞—Ç–µ–ª—å–Ω–æ "
        "—Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."
    ),
}
# Cost per message
COST_PER_MESSAGE = {
    "chat": 2,
    "ai_file": 3,
    "image": 5,
    "edit": 6,
}


async def get_gemini_models_info() -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö Gemini –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.
    """

    try:
        models = client_edit_image.models.list()
        lines = ["ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Gemini:\n"]

        for model in models:
            # –ò–º—è –º–æ–¥–µ–ª–∏ —Ç–µ–ø–µ—Ä—å –≤ –∞—Ç—Ä–∏–±—É—Ç–µ 'name'
            model_id = model.name.split("/")[-1]
            input_tokens = model.input_token_limit
            output_tokens = model.output_token_limit

            # –ù–æ–≤—ã–π –∞—Ç—Ä–∏–±—É—Ç 'supported_actions' –≤–º–µ—Å—Ç–æ
            # 'supported_generation_methods'
            methods = ", ".join(model.supported_actions)

            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            temp = (
                f"{model.temperature:.1f}"
                if hasattr(model, "temperature")
                and model.temperature is not None
                else "–Ω–µ –∑–∞–¥–∞–Ω–∞"
            )

            lines.append(
                f"üîπ *{model_id}*\n"
                f" –í—Ö–æ–¥: {input_tokens} —Ç–æ–∫–µ–Ω–æ–≤\n"
                f" –í—ã—Ö–æ–¥: {output_tokens} —Ç–æ–∫–µ–Ω–æ–≤\n"
                f" –ú–µ—Ç–æ–¥—ã: {methods}\n"
                f" –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temp}\n"
            )

        return "\n".join(lines)
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π Gemini: {str(e)}"


async def get_openai_models_info() -> str:
    try:
        # –£–ë–ò–†–ê–ï–ú await ‚Äî –≤—ã–∑–æ–≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π!
        models = client_image.models.list()
        lines = ["ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ OpenAI:\n"]
        for model in models:
            lines.append(f"üîπ `{model.id}`")
        return "\n".join(lines)
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞: `{e}`"


async def ask_gpt51_with_web_search(
    context_history: list,
    enable_web_search: bool = True,
) -> str:
    """
    –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å GPT-5.2 —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–æ–º.

    :param context_history: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
        [
            {"role": "system", "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"},
            {"role": "user", "content": "–í–æ–ø—Ä–æ—Å"}
        ]
    :param enable_web_search: –†–∞–∑—Ä–µ—à–∏—Ç—å –ª–∏ web search
    :return: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏
    """

    if not context_history:
        raise ValueError("context_history –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ–¥–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à—ë–Ω –ø–æ–∏—Å–∫
    tools = []
    if enable_web_search:
        tools.append(
            {
                "type": "web_search",
            }
        )
    try:
        response = client_chat.responses.create(
            model=MODELS["chat"],
            input=context_history,
            tools=tools,
            timeout=60,
        )

        # –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç
        return response.output_text.strip()

    except Exception as e:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ GPT:", e)
        raise


async def generate_image(prompt: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é DALL-E"""
    model_name = MODELS["image"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è DALL-E)
    prompt_tokens = token_utils.token_counter.count_openai_tokens(
        prompt, model_name
    )
    max_tokens = token_utils.get_token_limit(model_name)

    if prompt_tokens > max_tokens:
        # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        avg_token_size = 4  # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        max_chars = max_tokens * avg_token_size
        prompt = prompt[:max_chars]

    try:
        response = client_image.images.generate(
            model=model_name,
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        return response.data[0].url
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")


async def edit_image_with_gemini(
    original_image: io.BytesIO, prompt: str
) -> str:
    """–†–µ–¥–∞–∫—Ç–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gemini 2.5 Flash"""
    model_name = MODELS["edit"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
        prompt_tokens = token_utils.token_counter.count_openai_tokens(
            prompt, model_name
        )
        max_tokens = token_utils.get_token_limit(model_name)

        if prompt_tokens > max_tokens:
            # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            avg_token_size = 4  # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            max_chars = max_tokens * avg_token_size
            prompt = prompt[:max_chars]

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Gemini
        original_image.seek(0)
        image_bytes = original_image.read()
        image_base64 = base64.b64encode(image_bytes).decode('utf-8')
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Gemini
        gemini_prompt = f"""
        –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_base64[:100]}...
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤—ã–ø–æ–ª–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è: {prompt}
        –í–∞–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
        1. –í–Ω–µ—Å–∏ –∏–º–µ–Ω–Ω–æ —Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—à–µ–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        2. –°–æ—Ö—Ä–∞–Ω–∏ –æ–±—â–∏–π —Å—Ç–∏–ª—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        3. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω, —É—Ç–æ—á–Ω–∏ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        4. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–æ–º–ø—Ç –≤ Gemini
        response = client_edit_image.models.generate_content(
            model=model_name,
            contents=[gemini_prompt],
        )
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if hasattr(response, "candidates") and response.candidates:
            for part in response.candidates[0].content.parts:
                if hasattr(part, "inline_data"):
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    return part.inline_data.data
                elif hasattr(part, "text"):
                    # –ï—Å–ª–∏ Gemini –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    raise Exception(
                        f"""
                        –ò–ò –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:
                        {part.text}"""
                    )
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ
        raise Exception("Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ")
    except Exception as e:
        raise Exception(
            f"–û—à–∏–±–∫–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}"
        )


async def transcribe_voice(file_path: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Whisper API."""
    with open(file_path, "rb") as audio_file:
        transcription = client_chat.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
        )
    return transcription.text
