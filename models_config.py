"""Configuration for AI models used by the bot."""
import os
import io
from dotenv import load_dotenv
import google.generativeai as genai
from openai import OpenAI
import token_utils

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ .env
load_dotenv()

# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
OPENAI_API_KEY_CHAT = os.getenv("OPENAI_API_KEY")
OPENAI_API_KEY_IMAGE = os.getenv("OPENAI_API_KEY_IMAGE")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ OpenAI –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
client_chat = OpenAI(api_key=OPENAI_API_KEY_CHAT)
client_image = OpenAI(api_key=OPENAI_API_KEY_IMAGE)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Gemini
genai.configure(api_key=GEMINI_API_KEY)

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
MODELS = {
    "chat": "gpt-5.1",
    "image": "dall-e-3",
    "edit": "gemini-2.5-flash-preview-image",
    "ai_file": "gpt-5.1",
}

# Cost per message
COST_PER_MESSAGE = {
    "chat": 2,
    "image": 5,
    "edit": 6,
    "ai_file": 3,
}


async def get_gemini_models_info() -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö Gemini –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.
    """
    try:
        models = genai.list_models()
        lines = ["ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Gemini:\n"]
        for model in models:
            model_id = model.name.split("/")[-1]
            input_tokens = model.input_token_limit
            output_tokens = model.output_token_limit
            methods = ", ".join(model.supported_generation_methods)
            temp = (
                f"{model.temperature:.1f}"
                if model.temperature
                else "–Ω–µ –∑–∞–¥–∞–Ω–∞"
            )

            lines.append(
                f"üîπ *{model_id}*"
                f"\n   –í—Ö–æ–¥: `{input_tokens}` —Ç–æ–∫–µ–Ω–æ–≤"
                f"\n   –í—ã—Ö–æ–¥: `{output_tokens}` —Ç–æ–∫–µ–Ω–æ–≤"
                f"\n   –†–µ–∂–∏–º—ã: `{methods}`"
                f"\n   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: `{temp}`"
                f"\n"
            )
        return "\n".join(lines)
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π Gemini: `{str(e)}`"


async def get_openai_models_info() -> str:
    try:
        # –£–ë–ò–†–ê–ï–ú await ‚Äî –≤—ã–∑–æ–≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π!
        models = client_image.models.list()
        lines = ["ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ OpenAI:\n"]
        for model in models:
            lines.append(f"üîπ `{model.id}`")
        return "\n".join(lines)
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞: `{e}`"


def ask_gpt51_with_web_search(
    query: str, enable_web_search: bool = True
) -> str:
    """
    –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å GPT-5.1 —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.

    :param query: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞.
    :param enable_web_search:
        –ï—Å–ª–∏ True ‚Äî –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫.
        –ï—Å–ª–∏ False ‚Äî —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è, –±–µ–∑ –ø–æ–∏—Å–∫–∞.
    :return: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –µ–≥–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    model_name = "gpt-5.1"
    max_tokens = token_utils.get_token_limit(model_name)
    query_tokens = token_utils.token_counter.count_openai_tokens(
        query, model_name
    )

    if query_tokens > max_tokens:
        # –û–±—Ä–µ–∑–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        avg_token_size = 4  # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        max_chars = max_tokens * avg_token_size
        query = query[:max_chars]

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à—ë–Ω –ø–æ–∏—Å–∫
    tools = (
        [
            {
                "type": "web_search",
                # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å: —Ñ–∏–ª—å—Ç—Ä—ã, —è–∑—ã–∫, —Ä–µ–≥–∏–æ–Ω –∏ —Ç.–ø.
            }
        ]
        if enable_web_search
        else []
    )

    # –í—ã–±–æ—Ä –ø–æ–≤–µ–¥–µ–Ω–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    tool_choice = "auto" if enable_web_search else "none"

    response = client_chat.responses.create(
        model="gpt-5.1",  # –∏–ª–∏ "gpt-5.1-thinking"
        tools=tools,
        tool_choice=tool_choice,
        input=query,
        instructions=(
            "You are a helpful assistant. "
            "Use web search only when your knowledge may be outdated "
            "or when the user explicitly asks for fresh data."
        ),
        temperature=0.4,
        # include sources only if web search is enabled
        include=(
            ["web_search_call.action.sources"] if enable_web_search else []
        ),
    )

    return response.output_text


async def generate_image(prompt: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é DALL-E"""
    model_name = MODELS["image"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è DALL-E)
    prompt_tokens = token_utils.token_counter.count_openai_tokens(
        prompt, model_name
    )
    max_tokens = token_utils.get_token_limit(model_name)

    if prompt_tokens > max_tokens:
        # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        avg_token_size = 4  # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        max_chars = max_tokens * avg_token_size
        prompt = prompt[:max_chars]

    try:
        response = client_image.images.generate(
            model=model_name,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        return response.data[0].url
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")


async def edit_image_with_gemini(
    original_image: io.BytesIO, prompt: str
) -> str:
    """–†–µ–¥–∞–∫—Ç–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gemini 2.5 Flash"""
    model_name = MODELS["edit"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
        prompt_tokens = token_utils.token_counter.count_openai_tokens(
            prompt, model_name
        )
        max_tokens = token_utils.get_token_limit(model_name)

        if prompt_tokens > max_tokens:
            # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            avg_token_size = 4  # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            max_chars = max_tokens * avg_token_size
            prompt = prompt[:max_chars]

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Gemini
        original_image.seek(0)
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å Gemini
        model = genai.GenerativeModel(model_name)
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Gemini
        gemini_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤—ã–ø–æ–ª–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è: {prompt}
        –í–∞–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
        1. –í–Ω–µ—Å–∏ –∏–º–µ–Ω–Ω–æ —Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—à–µ–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        2. –°–æ—Ö—Ä–∞–Ω–∏ –æ–±—â–∏–π —Å—Ç–∏–ª—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        3. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω, —É—Ç–æ—á–Ω–∏ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        4. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–æ–º–ø—Ç –≤ Gemini
        response = model.generate_content(
            [
                gemini_prompt,
                {"mime_type": "image/png", "data": original_image.getvalue()},
            ]
        )
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if hasattr(response, "candidates") and response.candidates:
            for part in response.candidates[0].content.parts:
                if hasattr(part, "inline_data"):
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    return part.inline_data.data
                elif hasattr(part, "text"):
                    # –ï—Å–ª–∏ Gemini –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    raise Exception(
                        f"""
                        –ò–ò –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:
                        {part.text}"""
                    )
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ
        raise Exception("Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ")
    except Exception as e:
        raise Exception(
            f"–û—à–∏–±–∫–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò: {str(e)}"
        )


async def transcribe_voice(file_path: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Whisper API."""
    with open(file_path, "rb") as audio_file:
        transcription = client_chat.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
        )
    return transcription.text
