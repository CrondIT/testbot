"""Utility functions for handling user interactions,
messages, and edit modes."""

import os
import io
from PIL import Image
import google.generativeai as genai
import dbbot
import token_utils
import file_utils
import billing_utils
import models_config
from telegram import Update
from telegram.ext import ContextTypes
from telegram.helpers import escape_markdown
from global_state import (
    user_contexts,
    user_modes,
    user_edit_data,
    user_file_data,
    MAX_CONTEXT_MESSAGES,
)


async def download_and_convert_image(
    file_id: str, context: ContextTypes.DEFAULT_TYPE
) -> io.BytesIO:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ PNG
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –≤ –≤–∏–¥–µ BytesIO
    """
    file = await context.bot.get_file(file_id)
    image_data = io.BytesIO()
    await file.download_to_memory(out=image_data)
    image_data.seek(0)
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ PNG
    try:
        with Image.open(image_data) as img:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–¥–ª—è JPEG)
            if img.mode in ("P", "RGBA", "LA"):
                # –°–æ–∑–¥–∞–µ–º –±–µ–ª—ã–π —Ñ–æ–Ω –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é
                background = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode == "P":
                    img = img.convert("RGBA")
                background.paste(
                    img, mask=img.split()[-1] if img.mode == "RGBA" else None
                )
                img = background
            elif img.mode != "RGB":
                img = img.convert("RGB")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ PNG
            png_data = io.BytesIO()
            img.save(png_data, format="PNG", optimize=True)
            png_data.seek(0)
            return png_data
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        image_data.seek(0)
        return image_data


async def save_image_from_data(image_data: bytes, filename: str) -> str:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –±–∏–Ω–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"""
    file_path = f"{filename}.png"
    with open(file_path, "wb") as f:
        f.write(image_data)
    return file_path


def initialize_user_context(user_id: int, current_mode: str):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ä–µ–∂–∏–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    if user_id not in user_contexts:
        user_contexts[user_id] = {}

    if current_mode not in user_contexts[user_id]:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
        system_message = models_config.SYSTEM_PROMPTS.get(current_mode)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
        user_contexts[user_id][current_mode] = [
            {"role": "system", "content": system_message}
        ]


async def edit_image_with_gemini(
    original_image: io.BytesIO, prompt: str
) -> str:
    """–†–µ–¥–∞–∫—Ç–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gemini 2.5 Flash"""
    model_name = models_config.MODELS["edit"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
        prompt_tokens = token_utils.token_counter.count_openai_tokens(
            prompt, model_name
        )
        max_tokens = token_utils.get_token_limit(model_name)

        if prompt_tokens > max_tokens:
            # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            avg_token_size = 4  # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            max_chars = max_tokens * avg_token_size
            prompt = prompt[:max_chars]

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Gemini
        original_image.seek(0)
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å Gemini
        model = genai.GenerativeModel(model_name)
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Gemini
        gemini_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤—ã–ø–æ–ª–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è: {prompt}
        –í–∞–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
        1. –í–Ω–µ—Å–∏ –∏–º–µ–Ω–Ω–æ —Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—à–µ–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        2. –°–æ—Ö—Ä–∞–Ω–∏ –æ–±—â–∏–π —Å—Ç–∏–ª—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        3. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω, —É—Ç–æ—á–Ω–∏ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        4. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–æ–º–ø—Ç –≤ Gemini
        response = model.generate_content(
            [
                gemini_prompt,
                {"mime_type": "image/png", "data": original_image.getvalue()},
            ]
        )
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if hasattr(response, "candidates") and response.candidates:
            for part in response.candidates[0].content.parts:
                if hasattr(part, "inline_data"):
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    return part.inline_data.data
                elif hasattr(part, "text"):
                    # –ï—Å–ª–∏ Gemini –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    raise Exception(
                        f"""
                        –ò–ò –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:
                        {part.text}"""
                    )
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ
        raise Exception("Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ")
    except Exception as e:
        raise Exception(
            f"–û—à–∏–±–∫–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò: {str(e)}"
        )


async def handle_edit_mode(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    user_id: int,
    user_message: str,
    cost: int,
    balance: float,
):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Ä–µ–∂–∏–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å Gemini"""
    edit_data = user_edit_data.get(user_id, {})
    # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–ø—Ä–∞–≤–∏–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    if update.message.photo:
        await update.message.reply_text("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ PNG...")
        image_data = await download_and_convert_image(
            update.message.photo[-1].file_id, context
        )
        if edit_data.get("step") == "waiting_image":
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            user_edit_data[user_id]["original_image"] = image_data
            user_edit_data[user_id]["step"] = "waiting_prompt"
            await update.message.reply_text(
                "‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ PNG. "
                "–¢–µ–ø–µ—Ä—å –æ–ø–∏—à–∏—Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ "
                "(–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Gemini 2.5 Flash)."
            )
        return
    # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–ø—Ä–∞–≤–∏–ª —Ç–µ–∫—Å—Ç
    # –∏–ª–∏ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (—É–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–µ)
    elif user_message:
        # user_message is already processed (either from text or voice)
        if edit_data.get("step") == "waiting_prompt":
            await update.message.reply_text("üîÑ –†–µ–¥–∞–∫—Ç–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
            try:
                original_image = user_edit_data[user_id]["original_image"]
                # –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gemini
                edited_image_data = await edit_image_with_gemini(
                    original_image, user_message
                )
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                file_path = await save_image_from_data(
                    edited_image_data, f"edited_{user_id}"
                )
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                with open(file_path, "rb") as photo:
                    await update.message.reply_photo(
                        photo,
                        caption=f"–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {user_message}",
                    )
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                os.remove(file_path)
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                user_edit_data[user_id] = {
                    "step": "waiting_image",
                    "original_image": None,
                }

                # –°–ø–∏—Å—ã–≤–∞–µ–º –º–æ–Ω–µ—Ç—ã –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ª–æ–≥
                from billing_utils import check_user_coins, spend_coins

                user_data, coins, giftcoins, balance, cost = (
                    await check_user_coins(user_id, "edit", context)
                )
                spend_coins(
                    user_id,
                    cost,
                    coins,
                    giftcoins,
                    "edit",
                    user_message,
                    f"Image edited with prompt: {user_message}",
                )
            except Exception as e:
                await update.message.reply_text(f"‚ö†Ô∏è {str(e)}")
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                user_edit_data[user_id] = {
                    "step": "waiting_image",
                    "original_image": None,
                }
            return
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–µ –Ω–∞ —Ç–æ–º —à–∞–≥–µ
        await update.message.reply_text(
            "‚ùå –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."
        )
        return
    # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–ø—Ä–∞–≤–∏–ª —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ
    await update.message.reply_text(
        "‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ —Ç–µ–∫—Å—Ç."
    )


async def handle_ai_file_mode(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    user_id: int,
    user_message: str,
    cost: int,
    balance: float,
):
    """Handle the ai_file mode functionality separately"""
    from billing_utils import spend_coins

    # Check if the message contains a document
    if update.message.document:
        print("1. Check if the message contains a document")
        # Get the file
        file = await context.bot.get_file(update.message.document.file_id)

        # Determine file extension
        file_ext = file_utils.get_file_extension(
            update.message.document.file_name
        )
        if file_ext.lower() not in file_utils.SUPPORTED_EXTENSIONS:
            await update.message.reply_text(
                f"‚ùå–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç."
                f" –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: "
                f"{', '.join(file_utils.SUPPORTED_EXTENSIONS)}"
            )
            return

        # Download file
        file_path = (
            f"temp_file_{user_id}_{update.message.message_id}{file_ext}"
        )
        await file.download_to_drive(file_path)

        try:
            # Extract text from file
            print("2. Extract text from file")
            await update.message.reply_text("üìÑ –ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞...")

            extracted_text = await file_utils.process_uploaded_file(
                file_path, file_ext
            )

            # Store extracted text for later use
            if user_id not in user_file_data:
                user_file_data[user_id] = {}
            user_file_data[user_id]["extracted_text"] = extracted_text

            # Confirm extraction
            await update.message.reply_text(
                f"‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω! –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(extracted_text)} —Å–∏–º–≤. "
                "–¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º —Ñ–∞–π–ª–∞."
            )

            # Clean up temporary file
            os.remove(file_path)
        except Exception as e:
            # Clean up temporary file even if there's an error
            if os.path.exists(file_path):
                os.remove(file_path)

            await update.message.reply_text(
                f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}"
            )
            return
    # Check if the message contains a photo (for OCR of images)
    elif update.message.photo:
        # Get the highest resolution photo
        file = await context.bot.get_file(update.message.photo[-1].file_id)

        # Determine file extension - for photos sent as images,
        # assume it's an image file
        file_ext = ".jpg"  # Telegram converts photos to JPEG

        # Download file
        file_path = (
            f"temp_image_{user_id}_{update.message.message_id}{file_ext}"
        )
        await file.download_to_drive(file_path)

        try:
            # Extract text from image using OCR
            await update.message.reply_text(
                "üîç –í—ã–ø–æ–ª–Ω—è—é OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è..."
            )

            extracted_text = await file_utils.extract_text_from_image(
                file_path
            )

            # Store extracted text for later use
            if user_id not in user_file_data:
                user_file_data[user_id] = {}
            user_file_data[user_id]["extracted_text"] = extracted_text

            # Confirm extraction
            await update.message.reply_text(
                f"‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω! –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(extracted_text)} —Å–∏–º."
                "–¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
            )

            # Clean up temporary file
            os.remove(file_path)
        except Exception as e:
            # Clean up temporary file even if there's an error
            if os.path.exists(file_path):
                os.remove(file_path)

            await update.message.reply_text(
                f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}"
            )
            return
    elif (
        # This will be true for both text messages and voice-converted messages
        user_message
        and user_id in user_file_data
        and "extracted_text" in user_file_data[user_id]
    ):
        # Process the question about the file content
        # user_message is already processed (either from text or voice)
        extracted_text = user_file_data[user_id]["extracted_text"]
        model_name = models_config.MODELS.get("ai_file")

        max_tokens = token_utils.get_token_limit(model_name)

        print(f"model {model_name} max tokens {max_tokens}")

        # Calculate more conservative character
        # limit considering the full message with history
        # Reserve more tokens for context,
        # history, and response (2500 instead of 1500)
        reserved_tokens_for_context = 2500
        max_content_tokens = max_tokens - reserved_tokens_for_context

        # Calculate max characters based on estimated token size
        avg_token_size = 3  # Average size of a token in characters
        max_chars = min(
            len(extracted_text), max_content_tokens * avg_token_size
        )
        print(f"max_chars {max_chars}")
        if len(extracted_text) > max_chars:
            # Truncate the extracted text and inform the user
            truncated_extracted_text = extracted_text[:max_chars]
            await update.message.reply_text(
                f"üìù –û–±—ä–µ–º —Ñ–∞–π–ª–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç. –ò—Å–ø–æ–ª—å–∑—É—é –ø–µ—Ä–≤—É—é "
                f"—á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ ({max_chars} —Å–∏–º–≤–æ–ª–æ–≤) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."
            )
        else:
            truncated_extracted_text = extracted_text

        # Add file content to the user's question
        augmented_question = (
            f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:"
            f" {truncated_extracted_text}\n\n–í–æ–ø—Ä–æ—Å: {user_message}"
        )

        # First check if the augmented question itself is too long
        question_tokens = token_utils.token_counter.count_openai_tokens(
            augmented_question, model_name
        )
        print(f"question_tokens {question_tokens}")
        if question_tokens > max_content_tokens:
            # The combined content (file + question) exceeds token limits
            # Try to preserve as much of the file content
            # as possible and truncate the user's question

            # Calculate tokens used by file content and header
            content_and_header_text = (
                f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç: "
                f"{truncated_extracted_text}\n\n–í–æ–ø—Ä–æ—Å: "
                )
            content_and_header_tokens = (
                token_utils.token_counter.count_openai_tokens(
                    content_and_header_text, model_name
                )
            )

            # Available tokens for the user's question
            # (with buffer for response)
            available_for_question = (
                max_tokens - content_and_header_tokens - 500
            )  # buffer for response

            if available_for_question > 0:
                # Calculate max characters for the user's question
                max_question_chars = int(
                    available_for_question * avg_token_size
                )
                if len(user_message) > max_question_chars:
                    # Truncate the user's question to fit with the file content
                    truncated_user_message = user_message[:max_question_chars]
                    augmented_question = (
                        f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:"
                        f" {truncated_extracted_text}\n\n"
                        f" –í–æ–ø—Ä–æ—Å: {truncated_user_message}"
                    )
                    await update.message.reply_text(
                        f"–í–æ–ø—Ä–æ—Å —Å–æ–∫—Ä–∞—â–µ–Ω –¥–æ {len(truncated_user_message)} —Å."
                        f"–¥–ª—è —É–∫–ª–∞–¥—ã–≤–∞–Ω–∏—è –≤ –ª–∏–º–∏—Ç—ã –≤–º–µ—Å—Ç–µ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º —Ñ–∞–π–ª–∞."
                    )
                else:
                    # The issue might be with accumulated context history,
                    # not the question length
                    # We'll proceed with the original augmented question
                    # and let the later truncation handle it
                    pass
            else:
                # Not enough tokens even for the file content and header,
                # so truncate everything
                max_total_chars = max_content_tokens * avg_token_size
                augmented_question = augmented_question[:max_total_chars]
                await update.message.reply_text(
                    f"–û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞ (—Ñ–∞–π–ª+–≤–æ–ø—Ä–æ—Å) —Å–æ–∫—Ä–∞—â–µ–Ω"
                    f"–¥–æ {max_total_chars} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —É–∫–ª–∞–¥—ã–≤–∞–Ω–∏—è –≤ –ª–∏–º–∏—Ç—ã."
                )

        print(
            f"model {model_name} max tokens {max_tokens}"
            f"max_chars {max_chars} question_tokens {question_tokens}"
        )

        # Prepare messages with truncated history
        # using the augmented question
        truncated_history = token_utils.truncate_messages_for_token_limit(
            user_contexts[user_id]["ai_file"],
            model=model_name,
            reserve_tokens=reserved_tokens_for_context,
        )
        messages = truncated_history + [
            {"role": "user", "content": augmented_question}
        ]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏—Å—Ç–æ—Ä–∏–∏
        if len(messages) > MAX_CONTEXT_MESSAGES:
            messages = messages[-MAX_CONTEXT_MESSAGES:]

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∏–µ–Ω—Ç —á–∞—Ç–∞
            print("3. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∏–µ–Ω—Ç —á–∞—Ç–∞")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ - —ç—Ç–æ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if messages and messages[-1]["role"] == "user":
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
                print("4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π")
                token_counter = token_utils.token_counter
                total_tokens = token_counter.count_openai_messages_tokens(
                    messages, model_name
                )
                max_tokens = token_utils.get_token_limit(model_name)
                print(f"4 before send total {total_tokens} max {max_tokens}")
                if total_tokens > max_tokens:
                    print("5 –û–±—Ä–µ–∑–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–æ ... [truncated]")
                    # –û–±—Ä–µ–∑–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–æ ... [truncated]
                    messages = token_utils.truncate_messages_for_token_limit(
                        messages,
                        model=model_name,
                        reserve_tokens=reserved_tokens_for_context,
                    )

                    # Double-check token count and if still too long,
                    #  truncate the user message specifically
                    total_tokens = token_counter.count_openai_messages_tokens(
                        messages, model_name
                    )
                    print(
                        f"Double-check tokens before send {total_tokens}"
                        f"max tokens {max_tokens}"
                        f"message {messages}"
                    )
                    if (
                        total_tokens > max_tokens
                        and messages
                        and messages[-1]["role"] == "user"
                    ):
                        original_content = messages[-1]["content"]
                        remaining_tokens = max_tokens - (
                            total_tokens
                            - token_utils.token_counter.count_openai_tokens(
                                original_content, model_name
                            )
                        )
                        if remaining_tokens > 0:
                            max_content_chars = (
                                remaining_tokens * avg_token_size
                            )
                            messages[-1]["content"] = original_content[
                                :max_content_chars
                            ]
            print(f"6. model {model_name} {user_message}")
            # Prepare the full context including system message,
            # history and current query
            system_message = models_config.SYSTEM_PROMPTS.get("ai_file")
            full_context = (
                [{"role": "system", "content": system_message}]
                + truncated_history
                + [{"role": "user", "content": augmented_question}]
            )
            reply = models_config.ask_gpt51_with_web_search(
                context_history=full_context,
                enable_web_search=False,
            )

            # reply = response.choices[0].message.content

            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: –¥–æ–±–∞–≤–ª—è–µ–º –∏ –∑–∞–ø—Ä–æ—Å, –∏ –æ—Ç–≤–µ—Ç
            user_contexts[user_id]["ai_file"].append(
                {"role": "user", "content": augmented_question}
            )
            user_contexts[user_id]["ai_file"].append(
                {"role": "assistant", "content": reply}
            )

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã Markdown,
            # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫
            safe_reply = escape_markdown(reply, version=2)
            await send_long_message(
                update, safe_reply, parse_mode="MarkdownV2"
            )

            # –°–ø–∏—Å—ã–≤–∞–µ–º –º–æ–Ω–µ—Ç—ã –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ª–æ–≥
            from billing_utils import check_user_coins

            user_data, coins, giftcoins, balance, cost = (
                await check_user_coins(user_id, "ai_file", context)
            )
            spend_coins(
                user_id,
                cost,
                coins,
                giftcoins,
                "ai_file",
                user_message,
                safe_reply,
            )
        except Exception as e:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ "Message is too long" –∏ –¥—Ä—É–≥–∏—Ö
            error_msg = str(e)
            if "too long" in error_msg.lower() or "token" in error_msg.lower():
                # LOGGING ====================
                log_text = f"–û—à–∏–±–∫–∞ (ai_file): –°–æ–æ–±—â–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–µ: {str(e)}"
                dbbot.log_action(user_id, "ai_file", log_text, 0, balance)
                await update.message.reply_text(
                    "‚ö†Ô∏è –î–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (ai_file).C–æ–∫—Ä–∞—Ç–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞."
                )
            else:
                # LOGGING ====================
                log_text = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ ChatGPT: {str(e)}"
                dbbot.log_action(user_id, "ai_file", log_text, 0, balance)
                await update.message.reply_text(
                    "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ ChatGPT."
                )
    else:
        # If user hasn't uploaded a file yet but is in file analysis mode
        await update.message.reply_text(
            "üìÅ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. "
            "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: PDF, DOCX, TXT, XLSX, XLS"
        )


async def handle_image_mode(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    user_id: int,
    user_message: str,
    cost: int,
    coins: int,
    giftcoins: int,
    balance: float,
):
    """Handle the image mode functionality separately"""
    from billing_utils import spend_coins

    # –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    await update.message.reply_text("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
    try:
        image_url = await models_config.generate_image(user_message)
        await update.message.reply_photo(
            image_url, caption=f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {user_message}"
        )
        # –°–ø–∏—Å—ã–≤–∞–µ–º –º–æ–Ω–µ—Ç—ã –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ª–æ–≥
        spend_coins(user_id, cost, coins, giftcoins, "image", user_message, "")
    except Exception as e:
        # LOGGING ====================
        log_text = f"‚ö†Ô∏è {str(e)}"
        dbbot.log_action(user_id, "image", log_text, 0, balance)
        await update.message.reply_text(f"‚ö†Ô∏è {str(e)}")


async def send_long_message(update: Update, text: str, parse_mode: str = None):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, —Ä–∞–∑–±–∏–≤–∞—è –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏,
    –µ—Å–ª–∏ –æ–Ω–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç Telegram (4096 —Å–∏–º–≤–æ–ª–æ–≤)
    """
    # Telegram's message limit is 4096 characters
    TELEGRAM_MESSAGE_LIMIT = 4096

    if len(text) <= TELEGRAM_MESSAGE_LIMIT:
        # Message fits in a single message
        await update.message.reply_text(text, parse_mode=parse_mode)
        return

    # Split the message by paragraphs first to avoid breaking sentences
    paragraphs = text.split("\n")

    current_message = ""
    for paragraph in paragraphs:
        # Check if adding this paragraph would exceed the limit
        if len(current_message) + len(paragraph) + 1 <= TELEGRAM_MESSAGE_LIMIT:
            if current_message:
                current_message += "\n" + paragraph
            else:
                current_message = paragraph
        else:
            # Send the current message if it's not empty
            if current_message:
                await update.message.reply_text(
                    current_message, parse_mode=parse_mode
                )

            # If the single paragraph is too long, split it by sentences
            if len(paragraph) > TELEGRAM_MESSAGE_LIMIT:
                sentences = paragraph.split(". ")
                temp_message = ""
                for sentence in sentences:
                    if (
                        len(temp_message) + len(sentence) + 2
                        <= TELEGRAM_MESSAGE_LIMIT
                    ):
                        if temp_message:
                            temp_message += ". " + sentence
                        else:
                            temp_message = sentence
                    else:
                        if temp_message:
                            await update.message.reply_text(
                                temp_message + ".", parse_mode=parse_mode
                            )
                        temp_message = sentence

                # Add the last part if there's anything left
                if temp_message:
                    current_message = temp_message
                else:
                    current_message = ""
            else:
                current_message = paragraph

    # Send the remaining message if there's anything left
    if current_message:
        await update.message.reply_text(current_message, parse_mode=parse_mode)


async def handle_chat_mode(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    user_id: int,
    user_message: str,
    cost: int,
    coins: int,
    giftcoins: int,
    balance: float,
):
    """Handle the chat mode functionality separately"""
    print(f"we are in handle_chat_mode, user_message-{user_message}")
    from billing_utils import spend_coins

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º –¥–ª—è —Ä–µ–∂–∏–º–∞ chat
        # Include chat history for context with proper token limit
        model_name = models_config.MODELS.get("chat")
        user_context = []
        if user_id in user_contexts and "chat" in user_contexts[user_id]:
            # Create a temporary history that includes the current user message
            temp_history = user_contexts[user_id]["chat"] + [
                {"role": "user", "content": user_message}
            ]

            # Truncate history based on token limits,
            # including the current message
            user_context = token_utils.truncate_messages_for_token_limit(
                messages=temp_history,
                model=model_name,
                reserve_tokens=1500,
            )

        # Additionally limit the number of messages in history
        if len(user_context) > MAX_CONTEXT_MESSAGES:
            user_context = user_context[-MAX_CONTEXT_MESSAGES:]

        reply = models_config.ask_gpt51_with_web_search(
            enable_web_search=True,
            context_history=user_context,
        )

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: –¥–æ–±–∞–≤–ª—è–µ–º –∏ –∑–∞–ø—Ä–æ—Å, –∏ –æ—Ç–≤–µ—Ç
        user_contexts[user_id]["chat"].append(
            {"role": "user", "content": user_message}
        )
        user_contexts[user_id]["chat"].append(
            {"role": "assistant", "content": reply}
        )

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã Markdown, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫
        safe_reply = escape_markdown(reply, version=2)

        # Send the message, splitting if necessary
        # to respect Telegram's character limit
        await send_long_message(update, safe_reply, parse_mode="MarkdownV2")

        # –°–ø–∏—Å—ã–≤–∞–µ–º –º–æ–Ω–µ—Ç—ã –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ª–æ–≥
        spend_coins(
            user_id,
            cost,
            coins,
            giftcoins,
            "chat",
            user_message,
            safe_reply,
        )
    except Exception as e:
        # LOGGING ====================
        log_text = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ ChatGPT: {str(e)}"
        print(log_text)
        dbbot.log_action(user_id, "chat", log_text, 0, balance)
        await update.message.reply_text("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ ChatGPT.")


async def handle_voice_message(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    user_id: int,
    current_mode: str,
    balance: float,
):
    """Handle voice message transcription and return the transcribed text"""
    # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    voice_file = await context.bot.get_file(update.message.voice.file_id)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    file_path = f"voice_{user_id}_{update.message.message_id}.ogg"
    await voice_file.download_to_drive(file_path)

    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–∫—Å—Ç
        user_message = await models_config.transcribe_voice(file_path)
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.remove(file_path)
        return user_message
    except Exception as e:
        # LOGGING ====================
        log_text = "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."
        dbbot.log_action(user_id, current_mode, log_text, 0, balance)
        print("–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏:", e)
        await update.message.reply_text(
            "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."
        )
        return None


async def handle_message_or_voice(
    update: Update, context: ContextTypes.DEFAULT_TYPE
):

    user_id = update.effective_user.id
    # –ï—Å–ª–∏ —Ä–µ–∂–∏–º –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º —á–∞—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if user_id not in user_modes:
        user_modes[user_id] = "chat"

    current_mode = user_modes[user_id]
    print(f"we are in handle message or voice, mode {current_mode}")

    # --- start coins check ---
    user_data, coins, giftcoins, balance, cost = (
        await billing_utils.check_user_coins(user_id, current_mode, context)
    )
    if user_data is None:
        return  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –µ—Å–ª–∏ –º–æ–Ω–µ—Ç –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
    # --- end coins check ---

    # === –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –¢–ï–ö–£–©–ï–ì–û –†–ï–ñ–ò–ú–ê ===
    initialize_user_context(user_id, current_mode)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤—ã–º
    if update.message.voice:
        result = await handle_voice_message(
            update, context, user_id, current_mode, balance
        )
        if result is None:
            return  # Error occurred in voice handling
        user_message = result
    elif update.message.text:
        # –û–±—ã—á–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        user_message = update.message.text.strip()
    elif update.message.document or update.message.photo:
        # File or photo message - we'll pass empty string as user_message
        # and let the mode handler process the file
        user_message = ""
    else:
        return  # –ù–µ —Ç–µ–∫—Å—Ç, –Ω–µ –≥–æ–ª–æ—Å –∏ –Ω–µ —Ñ–∞–π–ª

    # Handle file uploads in file_analysis mode
    if current_mode == "ai_file":
        await handle_ai_file_mode(
            update,
            context,
            user_id,
            user_message,
            cost,
            balance,
        )
        return  # End here for file analysis mode

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∂–∏–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if current_mode == "edit":
        await handle_edit_mode(
            update, context, user_id, user_message, cost, balance
        )
        return

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
    if current_mode == "image":
        await handle_image_mode(
            update,
            context,
            user_id,
            user_message,
            cost,
            coins,
            giftcoins,
            balance,
        )
        return

    # –î–ª—è —Ä–µ–∂–∏–º–∞ chat –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–µ–±-–ø–æ–∏—Å–∫–∞
    if current_mode == "chat":
        print(f"we are in handle message or voice in mode {current_mode}")
        await handle_chat_mode(
            update,
            context,
            user_id,
            user_message,
            cost,
            coins,
            giftcoins,
            balance,
        )
        return
